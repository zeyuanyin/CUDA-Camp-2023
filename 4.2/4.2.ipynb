{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9d9d48",
   "metadata": {},
   "source": [
    "# CUDA编程模型---原子操作和规约"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a5067",
   "metadata": {},
   "source": [
    "### 规约操作\n",
    "\n",
    "![](reduction.png)\n",
    "\n",
    "----\n",
    "接下来我们完成下面的一个实例：  \n",
    "给定一个数组A，它包含10000000个int类型的元素，求他所有的元素之和：  \n",
    "输入：A[10000000]  \n",
    "输出：output（A中所有元素之和）  \n",
    "\n",
    "我用四种方法完成了kernel函数, 请大家根据课上讲的内容和下面代码示例, 完成这个实例\n",
    "\n",
    "```c++\n",
    "__global__ void sum_gpu_naive(int *in, int count, int *out)\n",
    "{\n",
    "    int tmp = 0;\n",
    "    for(int idx = blockDim.x * blockIdx.x + threadIdx.x; idx < count; idx += blockDim.x * gridDim.x)\n",
    "    {\n",
    "        atomicAdd(out, in[idx]);\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void _shared_2pass_sum_gpu(int *in, int count, int *out)\n",
    "{\n",
    "    __shared__ int ken[BLOCK_SIZE];\n",
    "    //grid_loop\n",
    "    int shared_tmp=0;\n",
    "    for(int idx = blockDim.x * blockIdx.x + threadIdx.x; idx < count; idx += blockDim.x * gridDim.x)\n",
    "    {\n",
    "        shared_tmp +=in[idx];\n",
    "    }\n",
    "    ken[threadIdx.x] = shared_tmp;\n",
    "    __syncthreads();\n",
    "\n",
    "    for(int total_threads = BLOCK_SIZE/2; total_threads>=1; total_threads/=2)\n",
    "    {\n",
    "        if(threadIdx.x < total_threads)\n",
    "        {\n",
    "            ken[threadIdx.x] = ken[threadIdx.x] + ken[threadIdx.x + total_threads]; \n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    // block_sum -> share memory[0]\n",
    "    if(blockIdx.x * blockDim.x < count)\n",
    "    {\n",
    "        if(threadIdx.x == 0)\n",
    "        {\n",
    "            out[blockIdx.x] = ken[0];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void _shared_atomic_sum_gpu(int *in, int count, int *out)\n",
    "{\n",
    "    __shared__ int ken[BLOCK_SIZE];\n",
    "    //grid_loop\n",
    "    int shared_tmp=0;\n",
    "    for(int idx = blockDim.x * blockIdx.x + threadIdx.x; idx < count; idx += blockDim.x * gridDim.x)\n",
    "    {\n",
    "        shared_tmp +=in[idx];\n",
    "    }\n",
    "    ken[threadIdx.x] = shared_tmp;\n",
    "    __syncthreads();\n",
    "\n",
    "    for(int total_threads = BLOCK_SIZE/2; total_threads>=1; total_threads/=2)\n",
    "    {\n",
    "        if(threadIdx.x < total_threads)\n",
    "        {\n",
    "            ken[threadIdx.x] = ken[threadIdx.x] + ken[threadIdx.x + total_threads]; \n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    // block_sum -> share memory[0]\n",
    "    if(blockIdx.x * blockDim.x < count)\n",
    "    {\n",
    "        if(threadIdx.x == 0)\n",
    "        {\n",
    "            atomicAdd(out, ken[0]);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void _shared_atomic_shuffle_sum_gpu(int *in, int count, int *out)\n",
    "{\n",
    "    __shared__ int ken[BLOCK_SIZE];\n",
    "    //grid_loop\n",
    "    int shared_tmp=0;\n",
    "    for(int idx = blockDim.x * blockIdx.x + threadIdx.x; idx < count; idx += blockDim.x * gridDim.x)\n",
    "    {\n",
    "        shared_tmp +=in[idx];\n",
    "    }\n",
    "    ken[threadIdx.x] = shared_tmp;\n",
    "    __syncthreads();\n",
    "\n",
    "    for(int total_threads = BLOCK_SIZE/2; total_threads>=32; total_threads/=2)\n",
    "    {\n",
    "        if(threadIdx.x < total_threads)\n",
    "        {\n",
    "            ken[threadIdx.x] = ken[threadIdx.x] + ken[threadIdx.x + total_threads]; \n",
    "        }\n",
    "        __syncthreads();\n",
    "    }\n",
    "    int val = ken[threadIdx.x];\n",
    "    for (int offset = 16; offset > 0; offset /= 2)\n",
    "    {\n",
    "        val += __shfl_down_sync(0xffffffff, val, offset);\n",
    "    }\n",
    "\n",
    "    if(blockIdx.x * blockDim.x < count)\n",
    "    {\n",
    "        if(threadIdx.x == 0)\n",
    "        {\n",
    "            atomicAdd(out, val);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "在[sum.cu](sum.cu)中完成上述实例，如果遇到困难，请参考[result_sum.cu](result_sum.cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/cuda/bin/nvcc sum.cu -o sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3851074",
   "metadata": {},
   "outputs": [],
   "source": [
    "!./sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6234ceb",
   "metadata": {},
   "source": [
    "课后作业:\n",
    "\n",
    "* 根据上面的示例的思路, 完成程序:找出数组中的最大值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788ca04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
